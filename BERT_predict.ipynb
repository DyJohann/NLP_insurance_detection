{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.2.0\n",
    "!pip install -q biopython\n",
    "!pip install -q sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'B-currency': 11,\n",
    " 'I-rate': 10,\n",
    " 'I-item': 8,\n",
    " 'I-benefit': 4,\n",
    " 'B-parameter': 5,\n",
    " 'B-Heading': 1,\n",
    " 'B-item': 7,\n",
    " 'I-Heading': 2,\n",
    " 'B-rate': 9,\n",
    " 'I-currency': 12,\n",
    " 'I-parameter': 6,\n",
    " 'B-benefit': 3,\n",
    " 'o': 0}\n",
    "\n",
    "key_list=list(label_map.keys())\n",
    "value_list=list(label_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_test_file():\n",
    "\n",
    "    test_file_list = list()\n",
    "    \n",
    "    for root, dirs, files in os.walk(\"new_data/data_process\"):\n",
    "        test_file_list = files\n",
    "        break\n",
    "    \n",
    "    print(\"find files below:\")\n",
    "    for i in range(len(test_file_list)):\n",
    "        print((i+1), \". \", test_file_list[i])\n",
    "    return test_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def take_tokens_and_labels(test_file):\n",
    "    sentences = []\n",
    "    tokens = []\n",
    "\n",
    "    with open(\"new_data/data_process/%s\" % test_file, newline = '') as lines:                                                                                                 \n",
    "\n",
    "        line_reader = csv.reader(lines, delimiter='\\t')\n",
    "\n",
    "        for line in line_reader:\n",
    "            if line == []:\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)      \n",
    "                tokens = []    \n",
    "            else: \n",
    "                if not line[0]==' ':\n",
    "                    tokens.append(line[0])\n",
    "                #if not line[1]==' ':\n",
    "                    #tokens.append(line[1])\n",
    "\n",
    "    return sentences, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "def retokenize(sentences):  \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        sent_str = ' '.join(sent)\n",
    "        encoded_dict = tokenizer.encode_plus(sent_str,\n",
    "                                             add_special_tokens = True,\n",
    "                                             max_length = 500,\n",
    "                                             pad_to_max_length = True,\n",
    "                                             return_attention_mask = True,\n",
    "                                             return_tensors = 'pt')\n",
    "   \n",
    "        input_ids.append(encoded_dict['input_ids'][0])\n",
    "        attention_masks.append(encoded_dict['attention_mask'][0])\n",
    "\n",
    "    return input_ids, attention_masks, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_to_new_labels(input_ids, label_map, tokenizer):\n",
    "    new_labels = []\n",
    "    null_label_id = -100\n",
    "\n",
    "    for sen in input_ids:\n",
    "        padded_labels = []\n",
    "        \n",
    "        for token_id in sen:\n",
    "            token_id = token_id.numpy().item()\n",
    "            if (token_id == tokenizer.pad_token_id) or \\\n",
    "                (token_id == tokenizer.cls_token_id) or \\\n",
    "                (token_id == tokenizer.sep_token_id):\n",
    "\n",
    "                padded_labels.append(null_label_id)\n",
    "            else:\n",
    "                padded_labels.append(label_map[label_str])\n",
    "\n",
    "        assert(len(sen) == len(padded_labels))    \n",
    "        new_labels.append(padded_labels)\n",
    "        \n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, SequentialSampler\n",
    "def creat_dataloader(input_ids, attention_masks):\n",
    "    pt_input_ids = torch.stack(input_ids, dim=0)\n",
    "    pt_attention_masks = torch.stack(attention_masks, dim=0)\n",
    " \n",
    "    batch_size = 16\n",
    "\n",
    "    prediction_data = TensorDataset(pt_input_ids, pt_attention_masks)\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "    \n",
    "    return prediction_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def prediction(prediction_dataloader, model):\n",
    "    model.eval()\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    model.cuda()\n",
    "\n",
    "    predictions , true_labels = [], []\n",
    "\n",
    "    for batch in prediction_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            result = model(b_input_ids, \n",
    "                          token_type_ids=None, \n",
    "                          attention_mask=b_input_mask,\n",
    "                          return_dict=True)\n",
    "\n",
    "        logits = result.logits      \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        predictions.append(logits)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(predictions):\n",
    "    all_predictions = np.concatenate(predictions, axis=0)\n",
    "    predicted_label_ids = np.argmax(all_predictions, axis=2)\n",
    "\n",
    "    return predicted_label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from Bio import File\n",
    "\n",
    "def export_result(sentences, value_list, test_file, predicted_label_ids, key_list):\n",
    "    cpy_sentences=copy.deepcopy(sentences)\n",
    "    \n",
    "    bio_file = open(\"new_data/output/%s\" % (\"AI_\" + test_file),'w', encoding = \"utf-8\")\n",
    "    for i in range(len(predicted_label_ids)):\n",
    "        ins_sentences=cpy_sentences[i]\n",
    "        ins_sentences.insert(0,'')\n",
    "\n",
    "        #for tl,pl,sen in zip(all_true_labels[i],predicted_label_ids[i],np.array(ins_sentences)):\n",
    "        for pl,sen in zip(predicted_label_ids[i],np.array(ins_sentences)):\n",
    "            #if not tl==-100:\n",
    "            if not sen==\"\":\n",
    "                bio_file.write('{0}\\t{1}\\n'.format(key_list[value_list.index(pl)], sen))\n",
    "        bio_file.write('\\n')\n",
    "    bio_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def evaluation(predicted_label_ids, all_true_labels):\n",
    "    \n",
    "    predicted_label_ids = np.concatenate(predicted_label_ids, axis=0)\n",
    "    all_true_labels = np.concatenate(all_true_labels, axis=0)\n",
    "\n",
    "    real_token_predictions = []\n",
    "    real_token_labels = []\n",
    "\n",
    "    for i in range(len(all_true_labels)):\n",
    "\n",
    "        if not all_true_labels[i] == -100:\n",
    "\n",
    "            real_token_predictions.append(predicted_label_ids[i])\n",
    "            real_token_labels.append(all_true_labels[i])\n",
    "\n",
    "    f1 = f1_score(real_token_labels, real_token_predictions, average='micro')\n",
    "    print(\"F1 score: {:.2%}\".format(f1))\n",
    "\n",
    "    inv_label_map = dict(zip(value_list, key_list))\n",
    "    \n",
    "    inv_real_token_labels = []\n",
    "    inv_real_token_predictions = []\n",
    "    for i in range(len(real_token_labels)):\n",
    "        inv_real_token_labels.append(inv_label_map[real_token_labels[i]])\n",
    "    for i in range(len(real_token_predictions)):\n",
    "        inv_real_token_predictions.append(inv_label_map[real_token_predictions[i]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    matrix = confusion_matrix(inv_real_token_labels, inv_real_token_predictions)\n",
    "    \n",
    "    new_value_list = []\n",
    "    for i in value_list:\n",
    "        if i in list(predicted_label_ids):\n",
    "            new_value_list.append(inv_label_map[i])\n",
    "\n",
    "    new_value_list.sort()\n",
    "    f, ax= plt.subplots(figsize = (18, 8))\n",
    "    ax.set_title('Confusion matrix', fontsize = 30)\n",
    "    sns.set(font_scale=1.5)\n",
    "    sns.heatmap(matrix, annot=True, fmt='g', cmap='Blues', xticklabels=new_value_list, yticklabels=new_value_list)\n",
    "    ax.invert_yaxis()\n",
    "    plt.xlabel(\"Predicted_label\", fontsize = 25)\n",
    "    plt.ylabel(\"Real_label\", fontsize = 25)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find files below:\n",
      "1 .  label_美年旺_0306C.bio\n",
      "2 .  label_美鑽奇積_new_c0306.bio\n",
      "3 .  label_ 利多桓年_0306C.bio\n",
      "4 .  label_樂退年年_0306C.bio\n",
      "5 .  data.txt\n",
      "6 .  label_民利旺_0306C.bio\n",
      "\n",
      "Start to predict label_美年旺_0306C.bio!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_美年旺_0306C.bio is done!\n",
      "\n",
      "Start to predict label_美鑽奇積_new_c0306.bio!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_美鑽奇積_new_c0306.bio is done!\n",
      "\n",
      "Start to predict label_ 利多桓年_0306C.bio!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_ 利多桓年_0306C.bio is done!\n",
      "\n",
      "Start to predict label_樂退年年_0306C.bio!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_樂退年年_0306C.bio is done!\n",
      "\n",
      "Start to predict label_民利旺_0306C.bio!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_民利旺_0306C.bio is done!\n",
      "\n",
      "-----Finish-----\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertForTokenClassification\n",
    "# model = BertForTokenClassification.from_pretrained('./model_save/', output_attentions=True)\n",
    "# assert model.config.output_attentions == True\n",
    "\n",
    "# test_file_list = load_test_file()\n",
    "# for test_file in test_file_list:\n",
    "#     file_format = test_file.split(\".\")[-1]\n",
    "#     if file_format == \"bio\":\n",
    "#         print(\"\\nStart to predict %s!\" % test_file)\n",
    "\n",
    "#         sentences, tokens = take_tokens_and_labels(test_file)\n",
    "\n",
    "#         input_ids, attention_masks, tokenizer = retokenize(sentences)\n",
    "\n",
    "#         prediction_dataloader = creat_dataloader(input_ids, attention_masks)\n",
    "\n",
    "#         predictions = prediction(prediction_dataloader, model)\n",
    "\n",
    "#         predicted_label_ids = flatten(predictions)\n",
    "\n",
    "#         export_result(sentences, value_list, test_file, predicted_label_ids, key_list)\n",
    "\n",
    "#         print(\"%s is done!\" % test_file)\n",
    "    \n",
    "# print(\"\\n-----Finish-----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
